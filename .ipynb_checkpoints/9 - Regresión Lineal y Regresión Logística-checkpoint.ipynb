{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal y Regresión Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aprendizaje Automático - Instituto de Computación - UdelaR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este módulo presentaremos dos métodos de aprendizaje supervisado: la regresión lineal, y el método de clasificación llamado regresión logística. Está basado fundamentalmente en las [notas del curso CS229](http://cs229.stanford.edu/notes/cs229-notes1.pdf) de la Universidad de Stanford, y de las presentaciones y material asociadas (disponibles a través de la plataforma Coursera). Sugerimos recurrir a ambas fuentes para más detalles respecto a los métodos aquí presentados. \n",
    "\n",
    "### 1. Regresión Lineal\n",
    "La regresión lineal es una forma de aprendizaje supervisado donde, a partir de un vector $x^T = (x_1, x_2, \\ldots, x_n)$ con $n$ _atributos_ (o _variables_) se busca construir una función (hipótesis) $h_{\\theta}(x): \\mathbb{R}^{n} \\to \\mathbb{R}$ que prediga la salida $y \\in \\mathbb{R}$ (llamada _variable o atributo de salida_), continua,  a través del siguiente modelo:\n",
    "\n",
    "$$h_{\\theta}(x) = \\theta_0+\\sum_{j=1}^n x_j\\theta_j$$\n",
    "\n",
    "A los elementos del vector $\\theta$ se lo conoce como _parámetros_ (también llamados _pesos_). Al término $\\theta_0$ se le llama _sesgo_, y usualmente se agrega una constante 1 al vector $x$, y se agrega $\\theta_0$ a $\\theta$, expresando entonces el modelo a través de un producto interno de vectores:\n",
    "\n",
    "$$h_{\\theta}(x)= x^T\\theta$$\n",
    "\n",
    "El problema de aprendizaje para la regresión lineal multivariada consiste en aprender los parámetros $\\theta$ a partir de un conjunto de entrenamiento $\\{(x^{(i)},y^{(i)})\\}$ que tiene $m$ elementos y donde cada $(x^{(i)},y^{(i)})$ es una _instancia_ de entrenamiento. Para esto, deberemos definir una función de costo que nos diga qué tan parecido es el valor predicho por $h_{\\theta}(x^{(i)})$ al verdadero valor de $y^{(i)}$ en el conjunto de entrenamiento. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos, por ejemplo, al \"Abalone dataset\" (un conjunto de datos que tiene como valores de entrada ciertas medidas de la caparazón de un molusco, y como salida el número de anillos):\n",
    "\n",
    "A continuación, se muestran algunos atributos del dataset, y algunas de sus instancias:\n",
    "\n",
    "| Largo|  Diámetro|  Altura|  Peso|  Anillos| \n",
    "| ------: |---:| -----:|---:|---:|\n",
    "| 0.455| 0.365| 0.095| 0.514| 15| \n",
    "| 0.35| 0.265| 0.09| 0.2255| 7| \n",
    "| 0.53| 0.42| 0.135| 0.677| 9| \n",
    "| 0.44| 0.365| 0.125| 0.516| 10| \n",
    "| 0.33| 0.255| 0.08| 0.205| 7| \n",
    "\n",
    "\n",
    "En este caso, el atributo \"Largo\" corresponde a $x_1$, \"Diámetro\" a $x_2$, y así sucesivamente. La instancia $(x^{(3)},y^{(3)})$, por ejemplo corresponde a $([1,0.53,0.42,0.135,0.677], 9)$, y por lo tanto $\\theta \\in \\mathbb{R}^5$. El problema de aprendizaje, en este caso, consiste en obtener, a partir de un conjunto de entrenamiento, un conjunto de valores para los elementos de $\\theta$, que permitan predecir, para nuevas instancias de $x$, su valor $y$ asociado, con tanta precisión como sea posible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mínimos Cuadrados\n",
    "\n",
    "Una método para estimar $\\theta$ es buscar aquellos valores que hagan que $h_\\theta(x)$ sea tan cercano a $y$ como sea posible, para las instancias de entrenamiento que contamos. Para esto, definiremos una _función de costo_, que mide esta diferencia, y que será la que intentemos minimizar. \n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Esta función (llamada de mínimos cuadrados), mide la diferencia entre cada valor de $y$ y el valor predicho por $h_\\theta(x)$, para el la instancia $x$ correspondiente, calcula su cuadrado (esto hace que siempre dé positivo), y hace la suma en todos los ejemplos de entrenamiento. La constante $\\frac{1}{2m}$ no afecta el resultado final... y hace más fáciles las operaciones al minimizar.\n",
    "\n",
    "Desde un punto de vista probabilistíco, la minimización de la función de mínimos cuadrados corresponde a encontrar, bajo ciertas condiciones, los estimadores de máxima verosimilitud (es decir, más adecuados al conjunto de entrenamiento) para $\\theta$. La justificación excede el alcance de este curso, pero vale mencionarlo para comenzar a formalizar la idea de que la elección de esta función de costo es, al menos, \"razonable\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ecuaciones Normales\n",
    "\n",
    "El objetivo, entonces, es obtener los valores de $\\theta$ que minimicen la función de costo $J(\\theta)$. La primera forma que veremos es directamente calcular las derivadas respecto a los diferentes $\\theta_j$ e igualarlas a 0 (al ser $J$ una función cuadrática, es también convexa, y por lo tanto solamente tiene un mínimo global, que coincide con el punto donde su gradiente  $\\nabla_\\theta$ es 0). \n",
    "\n",
    "Para esto, vamos primero a escribir $J$ en forma vectorial. Dado un conjunto de entrenamiento con $n$ atributos y $m$ instancias, definiremos la matriz de diseño $X \\in \\mathbb{R}^{m \\times (n+1)}$, como aquella que tiene las instancias de entrenamiento en sus filas, y al vector columna $y$ que tiene en cada fila el valor correspondiente de $y^{(i)}$. Puede verse que con esta formulación, llegamos a:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m}(X\\theta-y)^T(X\\theta -y)$$\n",
    "\n",
    "Utilizando propiedades de la traza de una matriz y sus gradientes, podemos llegar a un valor de $\\nabla_\\theta J(\\theta)$ (por el detalle de la derivación, consúltese las referencias):\n",
    "\n",
    "$$ \\nabla_\\theta J(\\theta) = X^TX\\theta - X^Ty $$\n",
    "\n",
    "Igualando el gradiente a 0, obtenemos las ecuaciones normales:\n",
    "\n",
    "$$ X^TX\\theta = X^Ty$$\n",
    "\n",
    "y por lo tanto el valor de $\\theta$ que minimiza $J(\\theta)$ estará dado por:\n",
    "\n",
    "$$ \\theta = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "Las ecuaciones normales proveen una forma cerrada de calcular los valores de $\\theta$ que minimizan $J(\\theta)$. El algoritmo asociado tiene $O(n^3)$, por lo que si el número de atributos o de instancias es muy grande, puede llegar a ser muy lento, y, en esos casos, es preferible utilizar métodos iterativos, como el que veremos a continuación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Descenso por gradiente\n",
    "\n",
    "El algoritmo de _descenso por gradiente_ es una aproximación completamente diferente a la minimización de $J(\\theta)$. Es un algoritmo de búsqueda iterativo, que parte de una estimación inicial de $\\theta$, y la va cambiando para que $J(\\theta)$ se reduzca, hasta converger a un valor de $\\theta$ que corresponde a un mínimo global de $J(\\theta)$. \n",
    "\n",
    "El algoritmo comienza con un $\\theta$ inicial, y repetidamente realiza la siguiente actualización (simultáneamente para todos los $\\theta_j$, con $j = 0,\\ldots,n$):\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) $$\n",
    "\n",
    "Para el caso de la minimización de la función de mínimos cuadrados, podemos hacer explícito el valor de $\\frac{\\partial}{\\partial \\theta_j}J(\\theta)$, a partir de su definición:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " \\frac{\\partial}{\\partial \\theta_j}J(\\theta)&=& \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y{(i)})^2 \\\\\n",
    " &=& \\frac{1}{2m}\\sum_{i=1}^{m} 2 \\cdot (h_\\theta(x^{(i)}) - y^{(i)})\\cdot \\frac{\\partial}{\\partial \\theta_j} (h_\\theta(x^{(i)}) - y^{(i)})\\\\\n",
    "&=& \\frac{1}{m}\\sum_{i=1}^{m}  (h_\\theta(x^{(i)}) - y^{(i)})\\cdot \\frac{\\partial}{\\partial \\theta_j} (\\sum_{p=0}^{n} \\theta_p x_p^{i} - y^{(i)})\\\\ \n",
    "&=& \\frac{1}{m}\\sum_{i=1}^{m}  (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j\\\\ \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, por lo tanto, la regla de actualización (simultánea para todos los $\\theta_j$) será:\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}  (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j   $$\n",
    "\n",
    "Esta regla (llamada LMS o de Widrow-Hoff) hace que la actualización de los valores de los parámetros $\\theta$ sea proporcional al error promedio cometido por la hipótesis actual, y en la dirección del gradiente (con el sentido opuesto). El algoritmo de _descenso por gradiente batch_ consiste en aplicar esta regla repetidamente, hasta lograr la convergencia (que podría definirse, por ejemplo, cuando $J(\\theta)$ queda por debajo de cierto valor $\\epsilon$).\n",
    "\n",
    "Puede verse que en este caso, para cada iteración se calcula el error cometido por la hipótesis sobre todas las instancias de entrenamiento. Una alternativa es actualizar los valores de $\\theta$ luego de calcular el error sobre cada ejemplo del conjunto de entrenamiento:\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\alpha (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j   \\text{   (simultáneamente para todos los $j$)} $$ \n",
    "\n",
    "En este caso, aunque el algoritmo no garantiza converger al mínimo, tiene la ventaja de hacerlo más rápido que la versión batch. Esta versión del algoritmo es conocida como _descenso por gradiente estocástico_ o _incremental_, y se utiliza especialmente en los casos en los que _m_ (es decir, la cantidad de instancias de entrenamiento) es muy grande.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Regresión Polinomial\n",
    "\n",
    "Si bien la función $h_\\theta(x)$ es lineal respecto a los valores de sus atributos, esto no quiere decir que tenga que ser necesariamente una recta respecto a los valores de entrada. La razón es que es posible definir atributos que sean combinaciones de los de entrada, como $x_1^2$ o $x_1x_2$, con los que la función $h_\\theta(x)$ será polinomial respecto a los atributos de entrada originales . La selección de estos atributos no es trivial, y dependerá del conocimiento del problema que tiene quien elabora la regresión (esto cambia mucho, y es una de sus virtudes, con los métodos basados en redes neuronales que veremos más adelante en el curso). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Descenso por Gradiente en la práctica\n",
    "\n",
    "Para poder aplicar descenso por gradiente de forma efectiva, deben tenerse algunos aspectos en cuenta:\n",
    "\n",
    "- **Selección de $\\alpha$ y criterio de convergencia**\n",
    "\n",
    "La constante $\\alpha$ que aparecen en la regla de Widrow-Hoff indica el tamaño del paso de reducción de $\\theta$ en la dirección indicada por el gradiente calculado. Cuanto más grande sea, más rápida será la convergencia. Sin embargo, si $\\alpha$ es demasiado grande, podemos dar un paso que haga que nos \"pasemos\"en nuestra aproximación al mínimo y que  el valor de $J(\\theta)$ comience a oscilar, o incluso a diverger (obsérvese que cada paso es proporcional a $\\alpha$, _pero también_ a la variable de entrada correspondiente). \n",
    "\n",
    "Una forma de ajustar $\\alpha$ es graficar $J(\\theta)$ versus el número de iteraciones del algoritmo: si el $\\alpha$ es adecuado, la convergencia debería ser rápida y el descenso de $J$ constante. Si no se da el primer caso, $\\alpha$ debería incrementarse. Si no se da el segundo ($J$ crece u oscila), $\\alpha$ debería reducirse.\n",
    "\n",
    "- **Escalado de atributos y normalización de la media**\n",
    "\n",
    "Cuando los diferentes atributos tienen valores en rangos muy diferentes, el descenso por gradiente convergerá más lentamente, porque $\\theta$ se reducirá mucho en los rangos más pequeños, pero poco en los grandes. Para evitar esto, lo usual es llevar los atributos de entrada a valores en los mismos rangos. \n",
    "\n",
    "En el escalado de atributos, cada valor de entrada se divide por el rango en que ese atributo aparece en el conjunto de entrenamiento (es decir, el valor máximo menos el mínimo). En la normalización de la media se resta a cada valor de un atributo de entrada el valor medio de ese atributo en el conjunto de entrenamiento. Aplicando ambos, se logra que todas los atributos queden en el rango $[-0.5,0.5]$ y tengan un valor medio de 0. La fórmula para ajustar cada atributo de la entrada es:\n",
    "$$\n",
    "x_i = \\frac{x_i - \\mu_i}{s_i}\n",
    "$$\n",
    "\n",
    "siendo $s_i$ el rango de la variable $x_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Sobreajuste y regularización\n",
    "\n",
    "En el caso de la regresión lineal, el sobreajuste podría hacer que la función $h_\\theta(x)$ sea muy compleja (por ejemplo, porque aparecen atributos de orden polinomial alto), y ajuste demasiado a los datos de entrenamiento, perdiendo capacidad de generalización. Una técnica usual (y que no solamente aplica para este método), es la de la *regularización*: se agrega un componente a la función de costo que busca penalizar cierto tipo de funciones (es decir, agrega _sesgo_ a nuestra hipótesis, para lograr menos _varianza_, y que pequeñas variaciones en los atributos de entrada no impliquen grandes cambios en la salida). En el caso de la regresión lineal, nuestra función de costo queda de la siguiente forma:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\left [ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y{(i)})^2 + \\lambda \\sum_{j=1}^n  \\theta_j^2 \\right ] $$\n",
    "\n",
    "Esta forma de regresión se conoce también como $Ridge$, y busca penalizar valores grandes de los parámetros. Esto es debido a que cuando los valores de algún $\\theta_j$ son muy grandes, pequeños cambios en la correspondiente variable $x_j$ producirán grandes cambios en el valor de $h_\\theta(x)$, haciendo que $h$ sea más proclive al sobreajuste. \n",
    "\n",
    "El parámetro $\\lambda$ cumple un rol muy importante: si es muy grande, el peso de tener una hipótesis \"simple\" (y por lo tanto nuestro sesgo) es mayor, mientras que si tiende a cero, intentaremos buscar hipótesis que se ajusten mejor a los datos de entrenamiento (aunque la varianza aumente). Por lo tanto, si $\\lambda$ es $0$, nuestro riesgo de sobreajuste es máximo, mientras que si $\\lambda$ tiende a infinito, entonces es probable que suframos de _underfitting_ (o sobregeneralización): nuestras hipótesis son tan sencillas que ajustaran mal incluso a los datos de entrenamiento. \n",
    "\n",
    "Aplicando el mismo razonamiento que cuando definimos la regla de actualización original, obtenemos nuestra nueva versión de descenso por gradiente, incluyendo regularización:\n",
    "\n",
    "\n",
    "$$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}  (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_0 $$\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\left [ \\left ( \\frac{1}{m}\\sum_{i=1}^{m}  (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j \\right \n",
    ") + \\frac{\\lambda}{m}\\theta_j  \\right ] \\text{   (simultáneamente para todos los $j \\in \\{1,2,\\ldots n\\}$)}$$\n",
    "\n",
    "Entonces, podemos ver que en cada iteración, el valor de cada $\\theta_j$ (excepto $\\theta_0$ que, por convención, no se penaliza) se multiplica por $\\left ( 1 - \\frac{\\lambda}{m} \\right )$, que siempre es menor que 1, y por lo tanto hace que su valor se reduzca.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Regresión Logística\n",
    "\n",
    "La regresión logística es un método de clasificación (igual que, por ejemplo, los árboles de decisión o los clasificadores bayesianos). La especificación es igual que en el caso de la regresión lineal, pero en este caso $y$, en lugar de tomar valores continuos, toma valores discretos. Empecemos por suponer que $y$ vale $0$ o $1$ (clasificación binaria), y luego veremos como generalizarlo.\n",
    "\n",
    "Una primera aproximación al problema podría ser ignorar que $y \\in {0,1}$, pero esto nos conduciría en muchos casos a resultados muy pobres. Otra opción es utilizar regresión lineal para obtener un estimador de la probabilidad $P(y=1 \\mid x;\\theta)$ (condicionada por $x$ y parametrizada por $\\theta$): \n",
    "\n",
    "$$ P(y=1 \\mid x;\\theta) = \\theta^T x$$\n",
    "\n",
    "El problema con esta aproximación es que el valor de $\\theta^T x$ está en el rango $(- \\infty, + \\infty)$, mientras que la probabilidad, por definición, debe estar entre 0 y 1. Para remediarlo, intentamos aproximar las chances (_odds_) en lugar de la probabilidad de $y=1$:\n",
    "\n",
    "$$ \\frac{P(y=1 \\mid x;\\theta)}{1-P(y=1 \\mid x;\\theta)} = \\theta^T x$$\n",
    "\n",
    "Mejor, pero no suficiente: las chances están en el rango $(0, + \\infty)$, por lo que estimaremos el logaritmo de las chances:\n",
    "\n",
    "$$ L \\left ( \\frac{P(y=1 \\mid x;\\theta)}{1-P(y=1 \\mid x;\\theta)} \\right )= \\theta^T x$$\n",
    "\n",
    "A partir de esto, podemos obtener la forma de nuestras hipótesis:\n",
    "\n",
    "$$ h_\\theta(x) = g(\\theta^Tx) = \\frac{1}{1+e^{-\\theta^Tx}}\\text{ , donde } g(z)= \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "$g(z)$ es conocida como la _función logística_ o _sigmoide_, y luce así:\n",
    "\n",
    "<img src=\"./1200px-Logistic-curve.svg.png\" alt=\"Sigmoide\" style=\"width: 200px;\"/>\n",
    "\n",
    "Esta función puede verse como una función de probabilidad continua, ya que tiende a $0$ cuando $z$ tiende a $-\\infty$, y tiende a $1$ cuando $z$ tiende a $+\\infty$. Además, es una función diferenciable, y su derivada tiene algunas propiedades que facilitan el cálculo. \n",
    "\n",
    "Entonces, interpretaremos a $h_\\theta(x)$ como la probabilidad de que $y=1$:\n",
    "\n",
    "$$ h_\\theta(x) = P(y=1 \\mid x;\\theta)$$\n",
    "\n",
    "Una forma de asignar una predicción al resultado de $h_\\theta(x)$ es predecir $y=1$ si $h_\\theta(x) \\geq 0.5$ e $y=0$ si $h_\\theta(x) < 0.5$. Como $g(z)\\geq 0.5$ siempre que $z \\geq 0$, entonces $h_\\theta(x) \\geq 0.5$ sii $\\theta^Tx \\geq 0$. Esto quiere decir que cada valor de $\\theta$ define una frontera de decisión lineal en el hiperplano correspondiente. \n",
    "\n",
    "Al igual que en el caso de la regresión lineal, la frontera de decisión no tiene por qué ser lineal respecto a las variables de entrada: basta con considerar atributos que son combinaciones de variables de entrada para obtener hipótesis polinomiales (o más complejas) respecto a las variables de entrada. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Función de costo para la regresión logística\n",
    "\n",
    "Dado un conjunto de entrenamiento ${(x^{(i)},y^{(i)}}$ donde $x_0^{(i)}=1$ para todo $i$, y con $y \\in {0,1}$ queremos obtener los valores de $\\theta$ que aproximen mejor a las instancias del conjunto. La función $J(\\theta)$ utilizada para la regresión lineal no es adecuada, ya que con la introducción de $g(z)$ en la hipótesis, deja de ser convexa, y por lo tanto presenta varios mínimos locales que impedirían el descenso por gradiente. Por lo tanto, construiremos una nueva función $J(\\theta)$, convexa, que luego intentaremos minimizar.\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m Cost(h_\\theta(x^{(i)}),y^{(i)})$$\n",
    "\n",
    "donde\n",
    "$$\\begin{align}\n",
    "Cost(h_\\theta(x),y) &=& -log(h_\\theta(x)) \\text{ si }y =1 \\\\\n",
    "Cost(h_\\theta(x),y) &=& -log(1- h_\\theta(x)) \\text{ si } y =0 \\end{align}$$\n",
    "\n",
    "Esta función de costo así construida tiene algunas propiedades interesantes: vale $0$ si el valor predicho por la hipótesis es igual a $y$, y tiende a infinito si difiere. Por lo tanto, está penalizando el error en la predicción de los valores de entrenamiento. Además, resulta ser convexa, por lo que es posible obtener el mínimo global utilizando métodos iterativos (descenso por gradiente, o métodos más avanzados como gradiente conjugado, BFGS, o L-BFGS).\n",
    "\n",
    "Una forma más compacta de expresar $J(\\theta)$ es la siguiente:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} log (h_\\theta(x^{(i)})) + (1 - y^{(i)} ) log (1 - h_\\theta(x^{(i)}))\\right ] \n",
    "$$\n",
    "\n",
    "o, en su versión vectorizada:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m \\left [ y^T log (h_\\theta(x)) + (1 - y^T ) log (1 - h_\\theta(x))\\right ] \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Descenso por gradiente para regresión logística\n",
    "\n",
    "Utilizando descenso por gradiente, y procediendo exactamente igual que como lo hicimos con la regresión lineal, obtenemos la regla de actualización siguiente:\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) $$\n",
    "\n",
    "Utilizando cálculo vectorial, esta regla se transfoma (para nuestra elección de $J(\\theta)$) en:\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}  (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j  \\text{, actualizando todos los } \\theta_j \\text{ simultáeamente} $$\n",
    "\n",
    "o, en su versión vectorial:\n",
    "\n",
    "$$ \\theta := \\theta - \\alpha \\frac{1}{m} X^T (g(X\\theta) - y)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Clasificación multiclase\n",
    "\n",
    "Una vez resuelto el problema de la regresión logística para clasificación binaria, lo extendermos al caso en que $y \\in \\{0,1,2 \\ldots n\\}$. Para ello dividiremos nuestro problema en $(n+1)$ problemas de clasificación binaria, donde en cada uno obtendremos un valor de $h_\\theta^{(i)}$ para el problema de separar las instancias de la clase $i$ del resto. Luego, dado un ejemplo $x$, prediremos la clase $i$ para la que el valor de $h_\\theta^{(i)}(x)$ es máximo:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y \\in \\{1,2\\ldots n\\}\\\\\n",
    "h_\\theta^{(i)} = P(y=i\\ \\mid x;\\theta) \\\\\n",
    "\\text{predecir  para } x = \\max_i (h_\\theta^{(i)})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Regularización \n",
    "\n",
    "Para evitar el sobreajuste, e igual que hicimos con la regresión lineal, modificamos la función de costo para penalizar los valores muy grandes de los parámetros:\n",
    "\n",
    "$$J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)}\\ \\log (h_\\theta (x^{(i)})) + (1 - y^{(i)})\\ \\log (1 - h_\\theta(x^{(i)}))\\right] + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_j^2$$\n",
    "\n",
    "Y obtenemos nuestra nueva regla de actualización para descenso por gradiente:\n",
    "\n",
    "$$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}  (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_0 $$\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\left [ \\left ( \\frac{1}{m}\\sum_{i=1}^{m}  (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j \\right \n",
    ") + \\frac{\\lambda}{m}\\theta_j  \\right ] \\text{   (simultáneamente para todos los $j \\in \\{1,2,\\ldots n\\}$)}  $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
